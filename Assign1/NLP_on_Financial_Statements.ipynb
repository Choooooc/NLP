{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import project_helper\n",
    "import project_tests\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "from selenium.webdriver import EdgeOptions\n",
    "options = EdgeOptions()\n",
    "options.add_argument(\"--headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S&P500 CIKs\n",
    "Filter the sp500_constituents csv by removing companies that were out after 2017.\n",
    "Use the sp500_constituents permnos to filter sp500_data and get a dictionary of tickers and\n",
    "CIKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "      permno       start      ending\n6      10104  1989-08-03  2022-03-31\n7      10107  1994-06-07  2022-03-31\n11     10138  1999-10-13  2022-03-31\n12     10145  1925-12-31  2022-03-31\n28     10299  2000-04-03  2017-03-10\n...      ...         ...         ...\n2008   93096  2012-12-03  2022-03-31\n2009   93132  2018-10-11  2022-03-31\n2011   93246  2021-03-22  2022-03-31\n2013   93429  2017-03-01  2022-03-31\n2014   93436  2020-12-21  2022-03-31\n\n[627 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>permno</th>\n      <th>start</th>\n      <th>ending</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>10104</td>\n      <td>1989-08-03</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10107</td>\n      <td>1994-06-07</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10138</td>\n      <td>1999-10-13</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>10145</td>\n      <td>1925-12-31</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>10299</td>\n      <td>2000-04-03</td>\n      <td>2017-03-10</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2008</th>\n      <td>93096</td>\n      <td>2012-12-03</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>2009</th>\n      <td>93132</td>\n      <td>2018-10-11</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>2011</th>\n      <td>93246</td>\n      <td>2021-03-22</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>2013</th>\n      <td>93429</td>\n      <td>2017-03-01</td>\n      <td>2022-03-31</td>\n    </tr>\n    <tr>\n      <th>2014</th>\n      <td>93436</td>\n      <td>2020-12-21</td>\n      <td>2022-03-31</td>\n    </tr>\n  </tbody>\n</table>\n<p>627 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500_constituents = pd.read_csv(\"sp500_constituents.csv\", dtype={\"permno\":int}, index_col=0)\n",
    "sp500_constituents = sp500_constituents[(sp500_constituents[\"ending\"] > \"2017-01-01\")]\n",
    "sp500_constituents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bwayn\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "sp500_data = pd.read_csv(\"sp500_w_addl_id_with_cik.csv\",dtype={\"cik\":str, \"permno\":int})\n",
    "sp500_data = sp500_data[[\"ticker\", \"permno\", \"cik\"]].set_index(\"ticker\")\n",
    "sp500_data = sp500_data[sp500_data[\"permno\"].isin(sp500_constituents[\"permno\"])]\n",
    "sp500_data.drop_duplicates(inplace=True)\n",
    "sp500_data.dropna(inplace=True)\n",
    "cik_lookup = sp500_data.to_dict()[\"cik\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWL request successful\n",
      "BBY request successful\n",
      "AIV request successful\n",
      "AXP request successful\n",
      "TIF request successful\n",
      "BAC request successful\n",
      "TGT request successful\n",
      "CVS request successful\n",
      "WFMI request successful\n",
      "ECL request successful\n",
      "PBCT request successful\n",
      "TAP request successful\n",
      "FLIR request successful\n",
      "TDC request successful\n",
      "DHI request successful\n",
      "GWW request successful\n",
      "WAT request successful\n",
      "CERN request successful\n",
      "SCG request successful\n",
      "PEP request successful\n",
      "IPG request successful\n",
      "BF request successful\n",
      "ADSK request successful\n",
      "AES request successful\n",
      "ED request successful\n",
      "LTD request successful\n",
      "IP request successful\n",
      "COF request successful\n",
      "CAT request successful\n",
      "OXY request successful\n",
      "STI request successful\n",
      "DOW request successful\n",
      "PEG request successful\n",
      "AIZ request successful\n",
      "RSG request successful\n",
      "CI request successful\n",
      "AKAM request successful\n",
      "GLW request successful\n",
      "FSLR request successful\n",
      "MCK request successful\n",
      "CAH request successful\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_38384/3032121072.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     42\u001B[0m                   'count': '10'}\n\u001B[0;32m     43\u001B[0m     \u001B[1;31m# request the url, and then parse the response.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m     \u001B[0mresponse_10q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mendpoint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparam_dict_10q\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m     \u001B[0msoup_10q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBeautifulSoup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse_10q\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'html.parser'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m     \u001B[0mdoc_table_10q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msoup_10q\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind_all\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'table'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclass_\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'tableFile2'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     73\u001B[0m     \"\"\"\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'get'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[1;31m# cases, and look like a memory leak in others.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0msessions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 61\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     62\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    540\u001B[0m         }\n\u001B[0;32m    541\u001B[0m         \u001B[0msend_kwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msettings\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 542\u001B[1;33m         \u001B[0mresp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    543\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    653\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    654\u001B[0m         \u001B[1;31m# Send the request\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 655\u001B[1;33m         \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    656\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    657\u001B[0m         \u001B[1;31m# Total elapsed time of the request (approximately)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    437\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    438\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mchunked\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 439\u001B[1;33m                 resp = conn.urlopen(\n\u001B[0m\u001B[0;32m    440\u001B[0m                     \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    441\u001B[0m                     \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    697\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    698\u001B[0m             \u001B[1;31m# Make the request on the httplib connection object.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 699\u001B[1;33m             httplib_response = self._make_request(\n\u001B[0m\u001B[0;32m    700\u001B[0m                 \u001B[0mconn\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    701\u001B[0m                 \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    443\u001B[0m                     \u001B[1;31m# Python 3 (including for exceptions like SystemExit).\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    444\u001B[0m                     \u001B[1;31m# Otherwise it looks like a bug in the code.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 445\u001B[1;33m                     \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    446\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mSocketTimeout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBaseSSLError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSocketError\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    447\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_raise_timeout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout_value\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mread_timeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001B[0m in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    438\u001B[0m                 \u001B[1;31m# Python 3\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    439\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 440\u001B[1;33m                     \u001B[0mhttplib_response\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetresponse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    441\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    442\u001B[0m                     \u001B[1;31m# Remove the TypeError from the exception chain in\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\http\\client.py\u001B[0m in \u001B[0;36mgetresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1369\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1370\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1371\u001B[1;33m                 \u001B[0mresponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbegin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1372\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mConnectionError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1373\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\http\\client.py\u001B[0m in \u001B[0;36mbegin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    317\u001B[0m         \u001B[1;31m# read until we get a non-100 response\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 319\u001B[1;33m             \u001B[0mversion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstatus\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreason\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_read_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    320\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mstatus\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mCONTINUE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    321\u001B[0m                 \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\http\\client.py\u001B[0m in \u001B[0;36m_read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    279\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_read_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 280\u001B[1;33m         \u001B[0mline\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_MAXLINE\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"iso-8859-1\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    281\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mline\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0m_MAXLINE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    282\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mLineTooLong\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"status line\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    702\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    703\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 704\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    705\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    706\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\ssl.py\u001B[0m in \u001B[0;36mrecv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1239\u001B[0m                   \u001B[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001B[0m \u001B[1;33m%\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1240\u001B[0m                   self.__class__)\n\u001B[1;32m-> 1241\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnbytes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1242\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1243\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbuffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnbytes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\ssl.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1097\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1098\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mbuffer\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1099\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sslobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1100\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1101\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sslobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "sec_api = project_helper.SecAPI()\n",
    "example_ticker = \"AMZN\"\n",
    "sec_data = {ticker: [] for ticker in cik_lookup}\n",
    "headers = {'Host': 'www.sec.gov', 'Connection': 'close',\n",
    "           'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "           'X-Requested-With': 'XMLHttpRequest',\n",
    "           'User-Agent': 'ruizhuoj@andrew.cmu.edu'\n",
    "           }\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "base_url_sec = r\"https://www.sec.gov\"\n",
    "for ticker in cik_lookup:\n",
    "    # define our parameters dictionary\n",
    "    param_dict_10k = {'action': 'getcompany',\n",
    "                  'CIK': cik_lookup[ticker],\n",
    "                  'type': '10-k',\n",
    "                  'dateb': '20220101',\n",
    "                  'owner': 'exclude',\n",
    "                  'start': '',\n",
    "                  'output': '',\n",
    "                  'count': '10'}\n",
    "    # request the url, and then parse the response.\n",
    "    response_10k = requests.get(url=endpoint, params=param_dict_10k, headers=headers)\n",
    "    soup_10k = BeautifulSoup(response_10k.content, 'html.parser')\n",
    "    doc_table_10k = soup_10k.find_all('table', class_='tableFile2')\n",
    "    param_dict_10q = {'action': 'getcompany',\n",
    "                  'CIK': cik_lookup[ticker],\n",
    "                  'type': '10-Q',\n",
    "                  'dateb': '20220101',\n",
    "                  'owner': 'exclude',\n",
    "                  'start': '',\n",
    "                  'output': '',\n",
    "                  'count': '20'}\n",
    "    # request the url, and then parse the response.\n",
    "    response_10q = requests.get(url=endpoint, params=param_dict_10q, headers=headers)\n",
    "    soup_10q = BeautifulSoup(response_10q.content, 'html.parser')\n",
    "    doc_table_10q = soup_10q.find_all('table', class_='tableFile2')\n",
    "    #Get 10-Ks\n",
    "    for row in doc_table_10k[0].find_all('tr'):\n",
    "        # find all the columns\n",
    "        cols = row.find_all('td')\n",
    "        # if there are no columns move on to the next row.\n",
    "        if len(cols) != 0:\n",
    "            # grab the text\n",
    "            filing_type = cols[0].text.strip()\n",
    "            filing_date = cols[3].text.strip()\n",
    "            if datetime.strptime(filing_date, '%Y-%m-%d').date() < datetime.strptime(\"2017\", '%Y').date():\n",
    "                pass\n",
    "            else:\n",
    "                filing_numb = cols[4].text.strip()\n",
    "                # find the links\n",
    "                filing_doc_href = cols[1].find('a', {'href': True, 'id': 'documentsbutton'})\n",
    "                filing_int_href = cols[1].find('a', {'href': True, 'id': 'interactiveDataBtn'})\n",
    "                filing_doc_link = base_url_sec + filing_doc_href['href']\n",
    "                sec_data[ticker].append((filing_doc_link, filing_type, filing_date))\n",
    "    #Get 10-Qs\n",
    "    for row in doc_table_10q[0].find_all('tr'):\n",
    "        # find all the columns\n",
    "        cols = row.find_all('td')\n",
    "        # if there are no columns move on to the next row.\n",
    "        if len(cols) != 0:\n",
    "            # grab the text\n",
    "            filing_type = cols[0].text.strip()\n",
    "            filing_date = cols[3].text.strip()\n",
    "            filing_numb = cols[4].text.strip()\n",
    "            # find the links\n",
    "            filing_doc_href = cols[1].find('a', {'href': True, 'id': 'documentsbutton'})\n",
    "            filing_int_href = cols[1].find('a', {'href': True, 'id': 'interactiveDataBtn'})\n",
    "            filing_doc_link = base_url_sec + filing_doc_href['href']\n",
    "            sec_data[ticker].append((filing_doc_link, filing_type, filing_date))\n",
    "    print(ticker, \"request successful\")\n",
    "pprint.pprint(sec_data[example_ticker][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download 10-ks\n",
    "As you see, this is a list of urls. These urls point to a file that contains metadata related to each filling. Since we don't care about the metadata, we'll pull the filling by replacing the url with the filling url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:   0%|          | 0/11 [00:00<?, ?filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872421000004/0001018724-21-000004-index.htm 10-K 2021-02-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:   9%|▉         | 1/11 [00:08<01:21,  8.12s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872421000028/0001018724-21-000028-index.htm 10-Q 2021-10-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  18%|█▊        | 2/11 [00:12<00:51,  5.70s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872421000020/0001018724-21-000020-index.htm 10-Q 2021-07-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  27%|██▋       | 3/11 [00:16<00:40,  5.07s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872421000010/0001018724-21-000010-index.htm 10-Q 2021-04-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  36%|███▋      | 4/11 [00:19<00:29,  4.28s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872420000030/0001018724-20-000030-index.htm 10-Q 2020-10-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  45%|████▌     | 5/11 [00:22<00:23,  3.92s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872420000021/0001018724-20-000021-index.htm 10-Q 2020-07-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  55%|█████▍    | 6/11 [00:26<00:19,  3.94s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872420000010/0001018724-20-000010-index.htm 10-Q 2020-05-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  64%|██████▎   | 7/11 [00:29<00:14,  3.68s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872419000089/0001018724-19-000089-index.htm 10-Q 2019-10-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  73%|███████▎  | 8/11 [00:33<00:11,  3.67s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872419000071/0001018724-19-000071-index.htm 10-Q 2019-07-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  82%|████████▏ | 9/11 [00:37<00:07,  3.61s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872419000043/0001018724-19-000043-index.htm 10-Q 2019-04-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings:  91%|█████████ | 10/11 [00:39<00:03,  3.25s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872418000159/0001018724-18-000159-index.htm 10-Q 2018-10-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings: 100%|██████████| 11/11 [00:42<00:00,  3.84s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:\n",
      "\n",
      "<SEC-DOCUMENT>0001018724-21-000004.txt : 20210203\n",
      "<SEC-HEADER>0001018724-21-000004.hdr.sgml : 20210203\n",
      "<ACCEPTANCE-DATETIME>20210202194410\n",
      "ACCESSION NUMBER:  0001018724-21-000004\n",
      "CONFORMED SUBMISSION TYPE: 10-K\n",
      "PUBLIC DOCUMENT COUNT:  107\n",
      "CONFORMED PERIOD OF REPORT: 20201231\n",
      "FILED AS OF DATE:  20210203\n",
      "DATE AS OF CHANGE:  20210202\n",
      "\n",
      "FILER:\n",
      "\n",
      " COMPANY DATA: \n",
      "  COMPANY CONFORMED NAME:   AMAZON COM INC\n",
      "  CENTRAL INDEX KEY:   0001018724\n",
      "  STANDARD INDUSTRIAL CLASSIFICATION: RETAIL-CATALOG & MAIL-ORDER HOUSES [5961]\n",
      "  IRS NUMBER:    911646860\n",
      "  STATE OF INCORPORATION:   DE\n",
      "  FISCAL YEAR END:   1231\n",
      "\n",
      " FILING VALUES:\n",
      "  FORM TYPE:  10-K\n",
      "  SEC ACT:  1934 Act\n",
      "  SEC FILE NUMBER: 000-22513\n",
      "  FILM NUMBER:  21583589\n",
      "\n",
      " BUSINESS ADDRESS: \n",
      "  STREET 1:  410 TERRY AVENUE NORTH\n",
      "  CITY:   SEATTLE\n",
      "  STATE:   WA\n",
      "  ZIP:   98109\n",
      "  BUSINESS PHONE:  2062661000\n",
      "\n",
      " MAIL ADDRESS: \n",
      "  STREET 1:  410 TERRY AVENUE NORTH\n",
      "  CITY:   SEATTLE\n",
      "  STATE:   WA\n",
      "  ZIP:   98109\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_fillings_by_ticker = {}\n",
    "browser = webdriver.Edge(service=EdgeService(EdgeChromiumDriverManager().install()))\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_fillings_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "        print(index_url, file_type, file_date)\n",
    "        if (file_type == '10-K' or file_type == '10-Q'):\n",
    "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')\n",
    "            browser.get(file_url)\n",
    "            elements = browser.find_element(By.TAG_NAME, \"body\")\n",
    "            raw_fillings_by_ticker[ticker][file_date] = elements.text\n",
    "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Documents\n",
    "With theses fillings downloaded, we want to break them into their associated documents. These documents are sectioned off in the fillings with the tags `<DOCUMENT>` for the start of each document and `</DOCUMENT>` for the end of each document. There's no overlap with these documents, so each `</DOCUMENT>` tag should come after the `<DOCUMENT>` with no `<DOCUMENT>` tag in between.\n",
    "\n",
    "Implement `get_documents` to return a list of these documents from a filling. Make sure not to include the tag in the returned document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_documents(text):\n",
    "    \"\"\"\n",
    "    Extract the documents from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text with the document strings inside\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    extracted_docs : list of str\n",
    "        The document strings found in `text`\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    extracted_docs = []\n",
    "    \n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')   \n",
    "    \n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "    \n",
    "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
    "            extracted_docs.append(text[doc_start_i:doc_end_i])\n",
    "    \n",
    "    return extracted_docs\n",
    "\n",
    "\n",
    "project_tests.test_get_documents(get_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_documents` function implemented, let's extract all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Documents from AMZN Fillings: 100%|██████████| 11/11 [00:00<00:00, 140.63filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 Filed on 2021-02-03:\n",
      "\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME>amzn-20201231.htm\n",
      "<DESCRIPTION>10-K\n",
      "<TEXT>\n",
      "<XBRL>\n",
      "<?xml version=\"1.0\" ?><!--XBRL Document Created with Wdesk from Workiva--><!--Copyright 2021 Workiva--><!--r:552820d...\n",
      "\n",
      "Document 1 Filed on 2021-02-03:\n",
      "\n",
      "<TYPE>EX-21.1\n",
      "<SEQUENCE>2\n",
      "<FILENAME>amzn-20201231xex211.htm\n",
      "<DESCRIPTION>EXHIBIT 21.1\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"><htm...\n",
      "\n",
      "Document 2 Filed on 2021-02-03:\n",
      "\n",
      "<TYPE>EX-23.1\n",
      "<SEQUENCE>3\n",
      "<FILENAME>amzn-20201231xex231.htm\n",
      "<DESCRIPTION>EXHIBIT 23.1\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"><htm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filling_documents_by_ticker = {}\n",
    "\n",
    "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "    filling_documents_by_ticker[ticker] = {}\n",
    "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
    "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
    "\n",
    "\n",
    "print('\\n\\n'.join([\n",
    "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "    for doc_i, doc in enumerate(docs)][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Document Types\n",
    "Now that we have all the documents, we want to find the 10-k form in this 10-k filing. Implement the `get_document_type` function to return the type of document given. The document type is located on a line with the `<TYPE>` tag. For example, a form of type \"TEST\" would have the line `<TYPE>TEST`. Make sure to return the type as lowercase, so this example would be returned as \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_document_type(doc):\n",
    "    \"\"\"\n",
    "    Return the document type lowercased\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "        The document string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_type : str\n",
    "        The document type lowercased\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    \n",
    "    doc_type = type_pattern.findall(doc)[0][len('<TYPE>'):] \n",
    "    \n",
    "    return doc_type.lower()\n",
    "\n",
    "\n",
    "project_tests.test_get_document_type(get_document_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_document_type` function, we'll filter out all non 10-k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2020123...\n",
      "    file_date: '2021-02-03'},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "ten_ks_by_ticker = {}\n",
    "\n",
    "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "    ten_ks_by_ticker[ticker] = []\n",
    "    for file_date, documents in filling_documents.items():\n",
    "        for document in documents:\n",
    "            if get_document_type(document) == '10-k':\n",
    "                ten_ks_by_ticker[ticker].append({\n",
    "                    'cik': cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date})\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "### Clean Up\n",
    "As you can see, the text for the documents are very messy. To clean this up, we'll remove the html and lowercase all the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `clean_text` function, we'll clean up all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning AMZN 10-Ks: 100%|██████████| 1/1 [00:02<00:00,  2.83s/10-K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20201231.htm\\n10-k\\n\\n\\n\\namzn-20...},\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_clean'] = clean_text(ten_k['file'])\n",
    "        with open('readme.txt', 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(ten_k['file_clean'])\n",
    "    project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize\n",
    "With the text cleaned up, it's time to distill the verbs down. Implement the `lemmatize_words` function to lemmatize verbs in the list of words provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"\n",
    "    Lemmatize words \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list of str\n",
    "        List of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lemmatized_words : list of str\n",
    "        List of lemmatized words\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "project_tests.test_lemmatize_words(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `lemmatize_words` function implemented, let's lemmatize all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pattern = re.compile('\\w+')\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))\n",
    "        print(ten_k['file_lemma'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]\n",
    "\n",
    "\n",
    "print('Stop Words Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ten_ks_by_ticker"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on 10ks\n",
    "### Loughran McDonald Sentiment Word Lists\n",
    "We'll be using the Loughran and McDonald sentiment word lists. These word lists cover the following sentiment:\n",
    "- Negative \n",
    "- Positive\n",
    "- Uncertainty\n",
    "- Litigious\n",
    "- Constraining\n",
    "- Superfluous\n",
    "- Modal\n",
    "\n",
    "This will allow us to do the sentiment analysis on the 10-ks. Let's first load these word lists. We'll be looking into a few of these sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']\n",
    "\n",
    "sentiment_df = pd.read_csv(os.path.join('..', '..', 'data', 'project_5_loughran_mcdonald', 'loughran_mcdonald_master_dic_2016.csv'))\n",
    "sentiment_df.columns = [column.lower() for column in sentiment_df.columns] # Lowercase the columns for ease of use\n",
    "\n",
    "# Remove unused information\n",
    "sentiment_df = sentiment_df[sentiments + ['word']]\n",
    "sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)\n",
    "sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]\n",
    "\n",
    "# Apply the same preprocessing to these words as the 10-k words\n",
    "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')\n",
    "\n",
    "\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "using the sentiment word lists, let's generate sentiment bag of words from the 10-k documents. Implement `get_bag_of_words` to generate a bag of words that counts the number of sentiment words in each doc. You can ignore words that are not in `sentiment_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_bag_of_words(sentiment_words, docs):\n",
    "    \"\"\"\n",
    "    Generate a bag of words from documents for a certain sentiment\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_words: Pandas Series\n",
    "        Words that signify a certain sentiment\n",
    "    docs : list of str\n",
    "        List of documents used to generate bag of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bag_of_words : 2-d Numpy Ndarray of int\n",
    "        Bag of words sentiment for each document\n",
    "        The first dimension is the document.\n",
    "        The second dimension is the word.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    vec = CountVectorizer(vocabulary=sentiment_words)\n",
    "    vectors = vec.fit_transform(docs)\n",
    "    words_list = vec.get_feature_names()\n",
    "    bag_of_words = np.zeros([len(docs), len(words_list)])\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "        bag_of_words[i] = vectors[i].toarray()[0]\n",
    "\n",
    "    return bag_of_words.astype(int)\n",
    "\n",
    "\n",
    "project_tests.test_get_bag_of_words(get_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `get_bag_of_words` function, we'll generate a bag of words for all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_bow_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
    "    \n",
    "    sentiment_bow_ten_ks[ticker] = {\n",
    "        sentiment: get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "        for sentiment in sentiments}\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data([sentiment_bow_ten_ks[example_ticker]], sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "Using the bag of words, let's calculate the jaccard similarity on the bag of words and plot it over time. Implement `get_jaccard_similarity` to return the jaccard similarities between each tick in time. Since the input, `bag_of_words_matrix`, is a bag of words for each time period in order, you just need to compute the jaccard similarities for each neighboring bag of words. Make sure to turn the bag of words into a boolean array when calculating the jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "\n",
    "def get_jaccard_similarity(bag_of_words_matrix):\n",
    "    \"\"\"\n",
    "    Get jaccard similarities for neighboring documents\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bag_of_words : 2-d Numpy Ndarray of int\n",
    "        Bag of words sentiment for each document\n",
    "        The first dimension is the document.\n",
    "        The second dimension is the word.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jaccard_similarities : list of float\n",
    "        Jaccard similarities for neighboring documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    jaccard_similarities = []\n",
    "    bag_of_words_matrix = np.array(bag_of_words_matrix, dtype=bool)\n",
    "    \n",
    "    for i in range(len(bag_of_words_matrix)-1):\n",
    "            u = bag_of_words_matrix[i]\n",
    "            v = bag_of_words_matrix[i+1]\n",
    "            jaccard_similarities.append(jaccard_similarity_score(u,v))    \n",
    "    \n",
    "    return jaccard_similarities\n",
    "\n",
    "\n",
    "project_tests.test_get_jaccard_similarity(get_jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `get_jaccard_similarity` function, let's plot the similarities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dates for the universe\n",
    "file_dates = {\n",
    "    ticker: [ten_k['file_date'] for ten_k in ten_ks]\n",
    "    for ticker, ten_ks in ten_ks_by_ticker.items()}  \n",
    "\n",
    "jaccard_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_jaccard_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "    for ticker, ten_k_sentiments in sentiment_bow_ten_ks.items()}\n",
    "\n",
    "\n",
    "project_helper.plot_similarities(\n",
    "    [jaccard_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Jaccard Similarities for {} Sentiment'.format(example_ticker),\n",
    "    sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "using the sentiment word lists, let's generate sentiment TFIDF from the 10-k documents. Implement `get_tfidf` to generate TFIDF from each document, using sentiment words as the terms. You can ignore words that are not in `sentiment_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def get_tfidf(sentiment_words, docs):\n",
    "    \"\"\"\n",
    "    Generate TFIDF values from documents for a certain sentiment\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment_words: Pandas Series\n",
    "        Words that signify a certain sentiment\n",
    "    docs : list of str\n",
    "        List of documents used to generate bag of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tfidf : 2-d Numpy Ndarray of float\n",
    "        TFIDF sentiment for each document\n",
    "        The first dimension is the document.\n",
    "        The second dimension is the word.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    vec = TfidfVectorizer(vocabulary=sentiment_words)\n",
    "    tfidf = vec.fit_transform(docs)\n",
    "    \n",
    "    return tfidf.toarray()\n",
    "\n",
    "\n",
    "project_tests.test_get_tfidf(get_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `get_tfidf` function, let's generate the TFIDF values for all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tfidf_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
    "    \n",
    "    sentiment_tfidf_ten_ks[ticker] = {\n",
    "        sentiment: get_tfidf(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "        for sentiment in sentiments}\n",
    "\n",
    "    \n",
    "project_helper.print_ten_k_data([sentiment_tfidf_ten_ks[example_ticker]], sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Using the TFIDF values, we'll calculate the cosine similarity and plot it over time. Implement `get_cosine_similarity` to return the cosine similarities between each tick in time. Since the input, `tfidf_matrix`, is a TFIDF vector for each time period in order, you just need to computer the cosine similarities for each neighboring vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def get_cosine_similarity(tfidf_matrix):\n",
    "    \"\"\"\n",
    "    Get cosine similarities for each neighboring TFIDF vector/document\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tfidf : 2-d Numpy Ndarray of float\n",
    "        TFIDF sentiment for each document\n",
    "        The first dimension is the document.\n",
    "        The second dimension is the word.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cosine_similarities : list of float\n",
    "        Cosine similarities for neighboring documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    cosine_similarities = []    \n",
    "    \n",
    "    for i in range(len(tfidf_matrix)-1):\n",
    "        cosine_similarities.append(cosine_similarity(tfidf_matrix[i].reshape(1, -1),tfidf_matrix[i+1].reshape(1, -1))[0,0])\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "project_tests.test_get_cosine_similarity(get_cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the cosine similarities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_cosine_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "    for ticker, ten_k_sentiments in sentiment_tfidf_ten_ks.items()}\n",
    "\n",
    "\n",
    "project_helper.plot_similarities(\n",
    "    [cosine_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Cosine Similarities for {} Sentiment'.format(example_ticker),\n",
    "    sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Alpha Factors\n",
    "Just like we did in project 4, let's evaluate the alpha factors. For this section, we'll just be looking at the cosine similarities, but it can be applied to the jaccard similarities as well.\n",
    "### Price Data\n",
    "Let's get yearly pricing to run the factor against, since 10-Ks are produced annually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing = pd.read_csv('../../data/project_5_yr/yr-quotemedia.csv', parse_dates=['date'])\n",
    "pricing = pricing.pivot(index='date', columns='ticker', values='adj_close')\n",
    "\n",
    "pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict to DataFrame\n",
    "The alphalens library uses dataframes, so we we'll need to turn our dictionary into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities_df_dict = {'date': [], 'ticker': [], 'sentiment': [], 'value': []}\n",
    "\n",
    "\n",
    "for ticker, ten_k_sentiments in cosine_similarities.items():\n",
    "    for sentiment_name, sentiment_values in ten_k_sentiments.items():\n",
    "        for sentiment_values, sentiment_value in enumerate(sentiment_values):\n",
    "            cosine_similarities_df_dict['ticker'].append(ticker)\n",
    "            cosine_similarities_df_dict['sentiment'].append(sentiment_name)\n",
    "            cosine_similarities_df_dict['value'].append(sentiment_value)\n",
    "            cosine_similarities_df_dict['date'].append(file_dates[ticker][1:][sentiment_values])\n",
    "\n",
    "cosine_similarities_df = pd.DataFrame(cosine_similarities_df_dict)\n",
    "cosine_similarities_df['date'] = pd.DatetimeIndex(cosine_similarities_df['date']).year\n",
    "cosine_similarities_df['date'] = pd.to_datetime(cosine_similarities_df['date'], format='%Y')\n",
    "\n",
    "\n",
    "cosine_similarities_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphalens Format\n",
    "In order to use a lot of the alphalens functions, we need to aligned the indices and convert the time to unix timestamp. In this next cell, we'll do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al\n",
    "\n",
    "\n",
    "factor_data = {}\n",
    "skipped_sentiments = []\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    cs_df = cosine_similarities_df[(cosine_similarities_df['sentiment'] == sentiment)]\n",
    "    cs_df = cs_df.pivot(index='date', columns='ticker', values='value')\n",
    "    \n",
    "    try:\n",
    "        data = al.utils.get_clean_factor_and_forward_returns(cs_df.stack(), pricing.loc[cs_df.index], quantiles=5, bins=None, periods=[1])\n",
    "        factor_data[sentiment] = data\n",
    "    except:\n",
    "        skipped_sentiments.append(sentiment)\n",
    "\n",
    "if skipped_sentiments:\n",
    "    print('\\nSkipped the following sentiments:\\n{}'.format('\\n'.join(skipped_sentiments)))\n",
    "factor_data[sentiments[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphalens Format with Unix Time\n",
    "Alphalen's `factor_rank_autocorrelation` and `mean_return_by_quantile` functions require unix timestamps to work, so we'll also create factor dataframes with unix time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unixt_factor_data = {\n",
    "    factor: data.set_index(pd.MultiIndex.from_tuples(\n",
    "        [(x.timestamp(), y) for x, y in data.index.values],\n",
    "        names=['date', 'asset']))\n",
    "    for factor, data in factor_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Returns\n",
    "Let's view the factor returns over time. We should be seeing it generally move up and to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor_name, data in factor_data.items():\n",
    "    ls_factor_returns[factor_name] = al.performance.factor_returns(data).iloc[:, 0]\n",
    "\n",
    "(1 + ls_factor_returns).cumprod().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Points Per Day per Quantile\n",
    "It is not enough to look just at the factor weighted return. A good alpha is also monotonic in quantiles. Let's looks the basis points for the factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor_name, data in unixt_factor_data.items():\n",
    "    qr_factor_returns[factor_name] = al.performance.mean_return_by_quantile(data)[0].iloc[:, 0]\n",
    "\n",
    "(10000*qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(5,3),\n",
    "    figsize=(14, 14),\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnover Analysis\n",
    "Without doing a full and formal backtest, we can analyze how stable the alphas are over time. Stability in this sense means that from period to period, the alpha ranks do not change much. Since trading is costly, we always prefer, all other things being equal, that the ranks do not change significantly per period. We can measure this with the **Factor Rank Autocorrelation (FRA)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_FRA = pd.DataFrame()\n",
    "\n",
    "for factor, data in unixt_factor_data.items():\n",
    "    ls_FRA[factor] = al.performance.factor_rank_autocorrelation(data)\n",
    "\n",
    "ls_FRA.plot(title=\"Factor Rank Autocorrelation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpe Ratio of the Alphas\n",
    "The last analysis we'll do on the factors will be sharpe ratio. Let's see what the sharpe ratio for the factors are. Generally, a Sharpe Ratio of near 1.0 or higher is an acceptable single alpha for this universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_annualization_factor = np.sqrt(252)\n",
    "\n",
    "(daily_annualization_factor * ls_factor_returns.mean() / ls_factor_returns.std()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've successfully done sentiment analysis on 10-ks!\n",
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "Python (base)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}